\documentclass[fleqn, a4paper, 12pt, twoside]{article}

\newcounter{recitationcount} %creates a new counter for recitation numbers (must be executed before exsheets is loaded)
\newcommand\recitation{\refstepcounter{recitationcount}}

\usepackage[counter-within = recitationcount]{exsheets} %question and solution environments
\usepackage{tasks}
\usepackage{amsmath, amssymb, amsthm} %standard AMS packages
\usepackage{esint} %integral signs
\usepackage{marginnote} %marginnotes
\usepackage{gensymb} %miscellaneous symbols
\usepackage{commath} %differential symbols
\usepackage{xcolor} %colours
\usepackage{cancel} %cancelling terms
\usepackage[free-standing-units]{siunitx} %formatting units
\usepackage{tikz, pgfplots} %diagrams
	\usetikzlibrary{calc, hobby, patterns, intersections, angles, quotes, spy}
\usepackage{graphicx} %inserting graphics
\usepackage{epstopdf} %converting and inserting eps graphics
\usepackage{hyperref} %hyperlinks
\usepackage{datetime} %date and time
\usepackage{ulem} %underline for \emph{}
\usepackage{xfrac, lmodern} %inline fractions
\usepackage{enumerate, enumitem} %numbered lists
\usepackage{float} %inserting floats
\usepackage[american voltages]{circuitikz} %circuit diagrams
\usepackage{pdflscape} %pages in landscape orientation
\usepackage{setspace} %double spacing
\usepackage{microtype} %micro-typography
\usepackage{listings} %formatting code
	\lstset{language=Matlab}
	\lstdefinestyle{standardMatlab}
	{
		belowcaptionskip=1\baselineskip,
		breaklines=true,
		frame=L,
		xleftmargin=\parindent,
		language=C,
		showstringspaces=false,
		basicstyle=\footnotesize\ttfamily,
		keywordstyle=\bfseries\color{green!40!black},
		commentstyle=\itshape\color{purple!40!black},
		identifierstyle=\color{blue},
		stringstyle=\color{orange},
	}
\usepackage{algpseudocode} %algorithms
\usepackage{algorithm} %algorithms

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %adds numbers to specific equations in non-numbered list of equations

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{law}{Law}

\newcommand{\curl}{\mathrm{curl\,}}

\newcommand{\divergence}{\mathrm{div\,}}

\DeclareMathOperator{\cond}{cond}

\makeatletter
\@addtoreset{section}{part} %resets section numbers in new part
\makeatother

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\renewcommand{\marginfont}{\scriptsize \color{blue}}

\renewcommand{\tilde}{\widetilde}

\RenewQuSolPair{question}[name=Recitation \therecitationcount\ -- Exercise]{solution}[name=Recitation \therecitationcount\ -- Solution]

\SetupExSheets{solution/print = true} %prints all solutions by default

%opening
\title{Numerical Analysis : Recitations}
\author{Aakash Jog}
\date{2015-16}

\begin{document}

\maketitle
%\setlength{\mathindent}{0pt}

\blfootnote
{	
	\begin{figure}[H]
		\includegraphics[height = 12pt]{cc.eps}
		\includegraphics[height = 12pt]{by.eps}
		\includegraphics[height = 12pt]{nc.eps}
		\includegraphics[height = 12pt]{sa.eps}
	\end{figure}
	This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy of this license, visit \url{http://creativecommons.org/licenses/by-nc-sa/4.0/}.
} %CC-BY-NC-SA license

\tableofcontents

\newpage
\section{Instructor Information}

\textbf{Ron Levie}\\
~\\
E-mail: \href{mailto:ronlevie@post.tau.ac.il}{ronlevie@post.tau.ac.il}\\

\recitation
\section{Errors}

\begin{definition}[Error]
	The absolute error in representation is defined as
	\begin{align*}
		e_x & = x - \tilde{x}
	\end{align*}
	The relative error in representation is defined as
	\begin{align*}
		\delta & = \frac{x - \tilde{x}}{x}
	\end{align*}
\end{definition}

\begin{question}
	The dimensions of a field are measured.
	The length is measured to be $\tilde{x} = 800 \metre$, with an absolute error bounded by 16.
	The width is measured to be $\tilde{y} = 30 \metre$, with an absolute error $e_y$, such that $|e_y| \le 6$.\\
	\begin{enumerate}
		\item Find the approximate bounds for $|\delta_x|$ and $|\delta_y|$.
		\item Find the bounds on the absolute error in the calculated area of the field.
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}[leftmargin=*]
		\item
			\begin{align*}
				|\delta_x|            & = \frac{|e_x|}{|x|}    \\
                                                      & \le \frac{16}{|x|}     \\
                                                      & \approx \frac{16}{800} \\
                                                      & = 0.02                 \\
				\therefore |\delta_x| & \le 0.02
			\end{align*}
		
			\begin{align*}
				|\delta_y|            & = \frac{|e_y|}{|y|}   \\
                                                      & \le \frac{6}{|y|}     \\
                                                      & \approx \frac{6}{300} \\
                                                      & = 0.02                \\
				\therefore |\delta_y| & \le 0.02
			\end{align*}
		\item
			The measured area of the field is
			\begin{align*}
				\tilde{A} & = \tilde{x} \tilde{y} \\
                                          & = 800 \cdot 300       \\
                                          & = 240000
			\end{align*}
			The maximum area of the field is
			\begin{align*}
				A_{\textnormal{max}} & = (\tilde{x} + {e_x}_{\textnormal{max}}) (\tilde{y} + {e_y}_{\textnormal{max}}) \\
                                                     & = (800 + 16) (300 + 6)                                                          \\
                                                     & = 249696
			\end{align*}
			The maximum area of the field is
			\begin{align*}
				A_{\textnormal{min}} & = (\tilde{x} + {e_x}_{\textnormal{min}}) (\tilde{y} + {e_y}_{\textnormal{min}}) \\
                                                     & = (800 - 16) (300 - 6)                                                          \\
                                                     & = 230496
			\end{align*}
			Therefore,
			\begin{align*}
				|e_{x y}| & \le (A_{\textnormal{max}} - A_{\textnormal{min}}) \\
                                          & \le 9696
			\end{align*}
		\item
			\begin{align*}
				|\delta_{x y}| & = \frac{|e_{x y}}{|x y|} \\
                                               & \le \frac{9696}{|x y|}   \\
                                               & \le \frac{9696}{230496}  \\
                                               & \approx 0.042
			\end{align*}
	\end{enumerate}
\end{solution}

\subsection{Propagation of Error}

\begin{question}
	Let $\tilde{x}$, $\tilde{y}$ be approximations of $x$, $y$.
	\begin{enumerate}
		\item Find a formula for the absolute error in $x + y$ in terms of $e_x$ and $e_y$.
		\item Find a formula for $\delta_{x + y}$, $\delta_{x - y}$ in terms of $\delta_x$, $\delta_y$, $x$, $y$.
		\item
			Let $\delta = \max \{\delta_x,\delta_y\}$.
			Assuming $x,y > 0$, show
			\begin{align*}
				|\delta_{x - y}| & \le \frac{x + y}{|x - y|} \delta
			\end{align*}
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}[leftmargin=*]
		\item
			\begin{align*}
				e_{x + y} & = (x + y) - (\tilde{x} + \tilde{y}) \\
                                          & = (x - \tilde{x}) + (y - \tilde{y}) \\
                                          & = e_x + e_y
			\end{align*}
		\item
			\begin{align*}
				\delta_{x + y} & = \frac{e_{x + y}}{x + y} \\
                                               & = \frac{e_x + e_y}{x + y} \\
                                               & = \frac{x \delta_x + y \delta_y}{x + y}
			\end{align*}
			Similarly,
			\begin{align*}
				\delta_{x - y} & = \frac{e_{x - y}}{x - y} \\
                                               & = \frac{e_x - e_y}{x - y} \\
                                               & = \frac{x \delta_x - y \delta_y}{x - y}
			\end{align*}
		\item
			\begin{align*}
				|\delta_{x - y}| & = \left| \frac{x \delta_x - y \delta_y}{x - y} \right| \\
                                                 & \le \frac{|x| |\delta_x| + |y| |\delta_y|}{|x - y|}    \\
                                                 & \le \frac{x \delta + y \delta}{|x - y|}                \\
                                                 & = \frac{x + y}{|x - y|} \delta
			\end{align*}
	\end{enumerate}
\end{solution}

\begin{question}
	Find a formula for $\delta_{x y}$, in terms of $x$, $y$, $\delta_x$, $\delta_y$.
\end{question}

\begin{solution}
	\begin{align*}
		\delta_a             & = \frac{a - \tilde{a}}{a} \\
		\therefore \tilde{a} & = a (1 - \delta_a)
	\end{align*}
	Therefore,
	\begin{align*}
		\tilde{x} \tilde{y} & = \left( x (1 - \delta_x) \right) \left( y (1 - \delta_y) \right) \\
                                    & = x y (1 - \delta_x - \delta_y + \delta_x \delta_y)               \\
	\end{align*}
	Also,
	\begin{align*}
		\tilde{x} \tilde{y} & = x y (1 - \delta_{x y})
	\end{align*}
	Therefore,
	\begin{align*}
		\delta_{x y} & = \delta_x + \delta_y - \delta_x \delta_y
	\end{align*}
\end{solution}

\recitation

\section{Interpolation by Polynomials}

\begin{theorem}[Existence and Uniqueness Theorem]
	There exists a unique polynomial $p_n(x)$ which approximates $f(x)$ between the sample points, i.e.
	\begin{align*}
		\left| e_n(x) \right| &= \left| f(x) - p_n(x) \right|
	\end{align*}
\end{theorem}

\begin{question}
	Find the interpolation polynomial for the data
	\begin{table}[H]
		\centering
		\begin{tabular}{c c}
			$x_i$ & $f(x_i)$\\
			$1$ & $3$\\
			$2$ & $2$\\
			$4$ & $1$\\
		\end{tabular}
	\end{table}
\end{question}

\begin{solution}
	Let
	\begin{align*}
		p(x) &= a_0 + a_1 x + a_2 x^2
	\end{align*}
	be the required interpolation polynomial.\\
	Therefore,
	\begin{align*}
		p(1) &= 3\\
		\therefore 3 &= a_0 + a_1 \cdot 1 + a_2 \cdot 1^2\\
		p(2) &= 2\\
		\therefore 2 &= a_0 + a_1 \cdot 2 + a_2 \cdot 2^2\\
		p(4) &= 1\\
		\therefore 1 &= a_0 + a_1 \cdot 4 + a_2 \cdot 4^2\\
	\end{align*}
	Therefore,
	\begin{align*}
			\begin{pmatrix}
				1 & 1 & 1^2\\
				1 & 2 & 2^2\\
				1 & 4 & 4^2\\
			\end{pmatrix}
			\begin{pmatrix}
				a_0\\
				a_1\\
				a_2\\
			\end{pmatrix}
		&=
			\begin{pmatrix}
				3\\
				2\\
				1\\
			\end{pmatrix}
	\end{align*}
	Therefore, solving,
	\begin{align*}
			\begin{pmatrix}
				a_0\\
				a_1\\
				a_2\\
			\end{pmatrix}
		&=
			\begin{pmatrix}
				\frac{13}{3}\\
				-\frac{3}{2}\\
				\frac{1}{6}\\
			\end{pmatrix}
	\end{align*}
	Therefore,
	\begin{align*}
		p(x) &= \frac{13}{3} - \frac{3}{2} x + \frac{1}{6} x^2
	\end{align*}
\end{solution}

\begin{definition}
	Let the sample points be $x_0,\dots,x_{n - 1}$.\\
	Lagrange polynomials are $n$ polynomials of degree $n - 1$, each of which is $0$ at all sample points, except one, at which it is $1$.\\
	\begin{align*}
		l_i(x) &= \frac{(x - x_0) \dots (x - x_{i - 1}) (x - x_{i + 1}) \dots (x - x_{n - 1})}{(x_i - x_0) \dots (x_i - x_{i - 1}) (x_i - x_{i + 1}) \dots (x_i - x_{n - 1})}
	\end{align*}
\end{definition}

\begin{question}
	Find the interpolation polynomial for the data
	\begin{table}[H]
		\centering
		\begin{tabular}{c c}
			$x_i$ & $f(x_i)$\\
			$1$ & $3$\\
			$2$ & $2$\\
			$4$ & $1$\\
		\end{tabular}
	\end{table}
	using Lagrange polynomials.
\end{question}

\begin{solution}
	Let
	\begin{align*}
		x_0 &= 1\\
		x_1 &= 2\\
		x_3 &= 4
	\end{align*}
	Therefore,
	\begin{align*}
		l_0(x) &= \frac{(x - x_1) (x - x_2)}{(x_0 - x_1) (x_0 - x_2)}\\
		&= \frac{(x - 2) (x - 4)}{(1 - 2) (1 - 4)}\\
		l_1(x) &= \frac{(x - x_0) (x - x_2)}{(x_1 - x_0) (x_1 - x_2)}\\
		&= \frac{(x - 1) (x - 4)}{(2 - 1) (2 - 4)}\\
		l_2(x) &= \frac{(x - x_0) (x - x_1)}{(x_2 - x_0) (x_2 - x_1)}\\
		&= \frac{(x - 1) (x - 2)}{(4 - 1) (4 - 2)}
	\end{align*}
	Therefore,
	\begin{align*}
		p(x) &= f(x_0) l_0(x) + f(x_1) l_1(x) + f(x_2) l_2(x)\\
		&= 3 l_0(x) + 2 l_1(x) + l_2(x)
	\end{align*}
\end{solution}

\recitation

\begin{question}
	Given
	\begin{table}[H]
		\centering
		\begin{tabular}{c c}
			$x_i$           & $f(x_i)$             \\
			$0$             & $0$                  \\
			$\frac{\pi}{4}$ & $\frac{\sqrt{2}}{2}$ \\
			$\frac{\pi}{2}$ & $1$                  \\
		\end{tabular}
	\end{table}
	Find the interpolating polynomial in Newton's form.
\end{question}

\begin{solution}
	The interpolating polynomial is
	\begin{align*}
		p_2(x) & = A_0 + A_1 (x - x_0) + A_2 (x - x_0) (x - x_1)
	\end{align*}
	where
	\begin{align*}
		A_k & = f[x_0,\dots,x_k]
	\end{align*}
	Therefore,
	\begin{align*}
		\therefore f[0]                          & = f(0)                          \\
                                                         & = 0                             \\
		\therefore f\left[ \frac{\pi}{4} \right] & = f\left( \frac{\pi}{4} \right) \\
                                                         & = \frac{\sqrt{2}}{2}            \\
		\therefore f\left[ \frac{\pi}{2} \right] & = f\left( \frac{\pi}{2} \right) \\
                                                         & = 1
	\end{align*}
	Therefore,
	\begin{align*}
		f\left[ 0,\frac{\pi}{4} \right]             & = \frac{f\left[ \frac{\pi}{4} \right] - f[0]}{\frac{\pi}{4} - 0}                                      \\
                                                            & = \frac{\frac{\sqrt{2}}{2} - 0}{\frac{\pi}{4}}                                                        \\
		f\left[ \frac{\pi}{4},\frac{\pi}{2} \right] & = \frac{f\left[ \frac{\pi}{2} \right] - f\left[ \frac{\pi}{4} \right]}{\frac{\pi}{2} - \frac{\pi}{4}} \\
	\end{align*}
	Therefore,
	\begin{align*}
		f\left[ 0,\frac{\pi}{4},\frac{\pi}{2} \right] & = \frac{f\left[ \frac{\pi}{4},\frac{\pi}{2} \right] - f\left[ 0,\frac{\pi}{4} \right]}{\frac{\pi}{2} - 0} \\
                                                              & = \frac{8 (1 - \sqrt{2})}{\pi^2}
	\end{align*}
	Therefore,
	\begin{align*}
		A_0 & = 0                      \\
		A_1 & = \frac{2 \sqrt{2}}{\pi} \\
		A_2 & = \frac{8 (1 - \sqrt{2})}{\pi^2}
	\end{align*}
	Therefore,
	\begin{align*}
		p_2(x) & = \frac{2 \sqrt{2}}{\pi} x + \frac{8 (1 - \sqrt{2})}{\pi^2} (x) \left( x - \frac{\pi}{4} \right)
	\end{align*}
\end{solution}

\begin{question}
	$\sin\left( \frac{\pi}{3} \right)$ was approximated using Newton's method, at sample points $0$, $\frac{\pi}{4}$, $\frac{\pi}{2}$, to be
	\begin{align*}
		p_2\left( \frac{\pi}{3} \right) & = \frac{2 \sqrt{2}}{3} + \frac{8 (1 - \sqrt{2})}{36} \\
                                                & = 0.8507
	\end{align*}
	Find the bounds on the error in this approximation.
\end{question}

\begin{solution}
	\begin{align*}
		|e_n(x)| & = \left| f(x) - p_n(x) \right| \\
                         & \le \left| \frac{f^{(n + 1)}(c)}{(n + 1)!} \prod\limits_{j = 0}^{n} (x - x_j) \right|
	\end{align*}
	where $c \in \left[ \min\{x_0,\dots,x_n,x\} , \max\{x_0,\dots,x_n,x\} \right]$.\\
	Therefore,
	\begin{align*}
		|e_2(x)|                                                  & \le \left| \frac{\sin^{(3)}(c)}{3!} \prod\limits_{j = 0}^{3} (x - x_j) \right|                                                                                         \\
		\therefore \left| e_2\left( \frac{\pi}{3} \right) \right| & \le \left| \frac{\sin^{(3)}(c)}{3!} \prod\limits_{j = 0}^{3} \left( \frac{\pi}{3} - x_j \right) \right|                                                                \\
                                                                          & \le \left| \frac{\sin^{(3)}(c)}{3!} \left( \frac{\pi}{3} - 0 \right) \left( \frac{\pi}{3} - \frac{\pi}{4} \right) \left( \frac{\pi}{3} - \frac{\pi}{2} \right) \right| \\
                                                                          & \le \left| \frac{-\cos(c)}{6} \frac{\pi^3}{(3) (12) (6)} \right|                                                                                                       \\
                                                                          & \le \left| \frac{-\cos(c) \pi^3}{1296} \right|
	\end{align*}
	Therefore, as $\left| \cos(c) \right|$ is bounded by $0$ and $1$,
	\begin{align*}
		\left| e_2\left( \frac{\pi}{3} \right) \right| & \le \left| \frac{\pi^3}{1296} \right| \\
                                                               & < 0.0242
	\end{align*}
\end{solution}

\recitation

\begin{question}
	Find Hermite's interpolating polynomial for the sample points $1$, $1$, $e$, for the function $f(x) = \ln(x)$.
\end{question}

\begin{solution}
	\begin{align*}
		f[x_0,\dots,x_k] &=
			\begin{cases}
				\frac{f[x_1,\dots,x_k] - f[x_0,\dots,x_{k - 1}]}{x_k - x_0} & ;\quad x_k \neq x_0 \\
				\frac{f^{(k)}(x_0)}{k!}                                     & ;\quad x_k = x_0    \\
			\end{cases}
	\end{align*}
	Therefore,
	\begin{align*}
		f[1] & = 0 \\
		f[1] & = 0 \\
		f[e] & = 1
	\end{align*}
	Therefore,
	\begin{align*}
		f[1,1] & = \frac{f'(1)}{1!}                   \\
                       & = \left. \frac{1}{x} \right|_{x = 1} \\
                       & = 1                                  \\
		f[1,e] & = \frac{1 - 0}{e - 1}                \\
                       & = \frac{1}{e - 1}
	\end{align*}
	Therefore,
	\begin{align*}
		f[1,1,e] & = \frac{\frac{1}{e - 1} - 1}{e - 1} \\
                         & = \frac{2 - e}{(e - 1)^2}
	\end{align*}
	Therefore,
	\begin{align*}
		p_2(x) & = f[1] + f[e] (x - 1) + \frac{2 - e}{(e - 1)^2} (x - 1) (x - 1) \\
                       & = 0 + 1 (x - 1) + \frac{2 - e}{(e - 1)^2} (x - 1)^2
	\end{align*}
\end{solution}

\section{Fixed Point Iterations and Root Finding}

\recitation
\begin{question}
	Show that
	\begin{align*}
		e_n & = \alpha - x_n \\
                    & \approx -\frac{f(x_n)}{f'(x_n)}
	\end{align*}
\end{question}

\begin{solution}
	By Lagrange's Mean Value Theorem, $\exists c \in (a,b)$, such that
	\begin{align*}
		f'(c) & = \frac{f(b) - f(a)}{b - a}
	\end{align*}
	Let
	\begin{align*}
		b & = x_n \\
		a & = \alpha
	\end{align*}
	Therefore,
	\begin{align*}
		\frac{f(x_n) - f(\alpha)}{x_n - \alpha} & = f'(c_n)
	\end{align*}
	where $c_n \in (\alpha,x_n)$.\\
	Therefore,
	\begin{align*}
		-e_n & = x_n - \alpha \\
                     & = \frac{f(x_n)}{f'(c_n)}
	\end{align*}
	Therefore, as $\lim\limits_{n \to \infty} x_n = 2$, for $n \to \infty$,
	\begin{align*}
		c_n & = x_n
	\end{align*}
	Therefore,
	\begin{align*}
		e_n & = -\frac{f(x_n)}{f'(x_n)}
	\end{align*}
\end{solution}

\begin{question}
	Let 
	\begin{align*}
		f(x) & = e^{-x} - \frac{1}{2}
	\end{align*}
	\begin{enumerate}
		\item Show that $f$ has a root in $[0,1]$.
		\item Show that Newton's method converges to the root $\alpha$ of $f$, and that $\alpha$ is unique.
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}[leftmargin=*]
		\item
			\begin{align*}
				f(0) & = e^0 - \frac{1}{2}           \\
                                     & = \frac{1}{2}                 \\
				f(1) & = \frac{1}{e} - \frac{1}{2}   \\
                                     & < \frac{1}{2.7} - \frac{1}{2} \\
                                     & < 0
			\end{align*}
			Therefore, by the intermediate value theorem, $\exists \alpha$ such that $f(\alpha) = 0$.
			Hence, $f$ has a root in $[0,1]$.
		\item
			\begin{align*}
				g(x) & = x - \frac{f(x)}{f'(x)}                  \\
                                     & = x + \frac{e^{-x} - \frac{1}{2}}{e^{-x}} \\
                                     & = x + 1 - \frac{1}{2} e^x
			\end{align*}
			Therefore,
			\begin{align*}
				g'(x) & = 1 - \frac{1}{2} e^x
			\end{align*}
			Therefore, as the extrema of $g$ are in $[0,1]$, $g : [0,1] \to [0,1]$.\\
			Similarly, $g'(x)$ is decreasing.\\
			Hence, by the fixed point theorem, as $\lim\limits_{n \to \infty} x_n = \alpha$, $\alpha$ is unique.
	\end{enumerate}
\end{solution}

\recitation
\begin{theorem}
	For the method $x_{n + 1} = g(x_n)$, if $\alpha = g(\alpha)$ and $\left| g'(\alpha) \right| < 1$, then $\exists$ a neighbourhood $(\alpha - \varepsilon , \alpha + \varepsilon) = \mathcal{N}$, of $\alpha$, such that for any $x_0 \in \mathcal{N}$,
	\begin{align*}
		\lim\limits_{n \to \infty} x_n & = \alpha
	\end{align*}
\end{theorem}

\begin{definition}[Rate of convergence]
	For a converging iterative method, $p$ is called the rate of convergence if $\exists c \neq 0$, such that
	\begin{align*}
		\lim\limits_{n \to \infty} \frac{|e_{n + 1}|}{|e_n|^p} & = c
	\end{align*}
	which is equivalent to
	\begin{align*}
		|e_{n + 1}| & = \left( c + \mathrm{o}(1) \right) |e_n|^p
	\end{align*}
	where $\mathrm{o}(1)$ is a sequence whose limit is $0$.
\end{definition}

\begin{theorem}
	Let $p \in \mathbb{N}$.
	If $g(\alpha) = \alpha$, and for $1 \le k < p$,
	\begin{align*}
		g^{(k)}(\alpha) & = 0
	\end{align*}
	and
	\begin{align*}
		g^{(p)}(\alpha) & \neq 0
	\end{align*}
	then ,the rate of convergence if $p$.
\end{theorem}

\begin{question}
	Consider the following iteration for calculating $\alpha = r^{\frac{1}{3}}$, where $r > 0$.
	\begin{align*}
		g(x)      & = A x + B r x^{-2} + C r^2 x^{-5} \\
		x_{n + 1} & = g(x_n)
	\end{align*}
	where $A,B,C \in \mathbb{R}$.
	\begin{enumerate}
		\item Find $A$, $B$, $C$, such that the method converges to $r^{\frac{1}{3}}$ with maximum rate of convergence.
		\item What is the rate of convergence?
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}[leftmargin=*]
		\item
			For the method to converge to $r^{\frac{1}{3}}$, $r^{\frac{1}{3}}$ must be a fixed point of $g$.\\
			Therefore,
			\begin{align*}
				g\left( r^{\frac{1}{3}} \right) & = A r^{\frac{1}{3}} + B r r^{-\frac{2}{3}} + C r^2 r^{-\frac{5}{3}} \\
				\therefore r^{\frac{1}{3}}      & = A r^{\frac{1}{3}} + B r^{\frac{1}{3}} + C r^{\frac{1}{3}}
			\end{align*}
			For the rate of convergence to be maximum,
			\begin{align*}
				g'\left( r^{\frac{1}{3}} \right) & = 0 \\
				\therefore A - 2 B - 5 C         & = 0
			\end{align*}
			Also, for the rate of convergence to maximum,
			\begin{align*}
				g''\left( r^{\frac{1}{3}} \right) & = 0 \\
				\therefore 6 B + 30 C             & = 0
			\end{align*}
			Therefore, solving,
			\begin{align*}
				A & = \frac{5}{9} \\
				B & = \frac{5}{9} \\
				C & = -\frac{1}{9}
			\end{align*}
			Therefore, the rate of convergence is greater than $2$.
		\item
			\begin{align*}
				g'''(x) & = -24 B r x^{-5} - 210 C r^2 x^{-8}
			\end{align*}
			Therefore,
			\begin{align*}
				g'''\left( r^{\frac{1}{3}} \right) & = \frac{40}{3} e^{-\frac{2}{3}} \\
                                                                   & \neq 0
			\end{align*}
			Therefore the rate of convergence is $3$.
	\end{enumerate}
\end{solution}

\recitation
\section{LU Decomposition and Norms}

\begin{question}
	Let
	\begin{align*}
		A &=
			\begin{pmatrix}
				1           & 2 & 3 \\
				1           & 2 & 4 \\
				\frac{1}{2} & 0 & 3 \\
			\end{pmatrix}
	\end{align*}
	\begin{enumerate}
		\item
			Find the PLU decomposition, i.e. the LU decomposition with pivoting, of $A$.
		\item
			Represent $P$ as a permutation vector.
		\item
			Use the decomposition to solve ${A x = b}$ for
			\begin{align*}
				b &=
					\begin{pmatrix}
						5 \\
						4 \\
						7 \\
					\end{pmatrix}
			\end{align*}
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}[leftmargin=*]
		\item
			\begin{align*}
				A &=
					\begin{pmatrix}
						1   & 2 & 3 \\
						1   & 2 & 4 \\
						0.5 & 0 & 3 \\
					\end{pmatrix}\\
				&\xrightarrow[m_{2 1} = 1 , m_{3 1} = 0.5]{R_2 \to R_2 - 1 R_1 , R_3 \to R_3 - \frac{1}{2} R_2}
					\begin{pmatrix}
						1 & 2  & 3   \\
						0 & 0  & 1   \\
						0 & -1 & 1.5 \\
					\end{pmatrix}\\
				&\xrightarrow[m_{2 1} = 0.5 , m_{3 1} = 1]{R_3 \leftrightarrow R_2}
					\begin{pmatrix}
						1 & 2  & 3   \\
						0 & -1 & 1.5 \\
						0 & 0  & 1   \\
					\end{pmatrix}
			\end{align*}
			Therefore,
			\begin{align*}
				P &=
					\begin{pmatrix}
						1 & 0 & 0 \\
						0 & 0 & 1 \\
						0 & 1 & 0 \\
					\end{pmatrix}\\
				L &=
					\begin{pmatrix}
						1       & 0       & 0 \\
						m_{2 1} & 1       & 0 \\
						m_{3 1} & m_{3 2} & 1 \\
					\end{pmatrix}\\
				&=
					\begin{pmatrix}
						1   & 0 & 0 \\
						0.5 & 1 & 0 \\
						1   & 0 & 1 \\
					\end{pmatrix}\\
				U &=
					\begin{pmatrix}
						1 & 2  & 3   \\
						0 & -1 & 1.5 \\
						0 & 0  & 1   \\
					\end{pmatrix}
			\end{align*}
		\item
			\begin{align*}
				P &=
					\begin{pmatrix}
						1 & 0 & 0 \\
						0 & 0 & 1 \\
						0 & 1 & 0 \\
					\end{pmatrix}
			\end{align*}
			Therefore, the corresponding permutation vector is
			\begin{align*}
				V &=
					\begin{pmatrix}
						1 \\
						3 \\
						2 \\
					\end{pmatrix}
			\end{align*}
		\item
			Using $V$,
			\begin{align*}
				B &\to
					\begin{pmatrix}
						5 \\
						7 \\
						4 \\
					\end{pmatrix}
			\end{align*}
			Therefore,
			\begin{align*}
				A x              & = b \\
				\therefore L U x & = b
			\end{align*}
			Let
			\begin{align*}
				U x & = y
			\end{align*}
			Therefore,
			\begin{align*}
				L y &= b\\
				\therefore 
					\begin{pmatrix}
						1   & 0 & 0 \\
						0.5 & 1 & 0 \\
						1   & 0 & 1 \\
					\end{pmatrix}
					\begin{pmatrix}
						y_1 \\
						y_2 \\
						y_3 \\
					\end{pmatrix}
				&=
					\begin{pmatrix}
						5 \\
						7 \\
						4 \\
					\end{pmatrix}
			\end{align*}
			Therefore,
			\begin{align*}
				y_1           & = 5 \\
				0.5 y_1 + y_2 & = 7 \\
				y_1 + y_3     & = 4
			\end{align*}
			Therefore, solving,
			\begin{align*}
				y_1 & = 5   \\
				y_2 & = 4.5 \\
				y_3 & = -1
			\end{align*}
			Therefore,
			\begin{align*}
				U x &= y\\
				\therefore
					\begin{pmatrix}
						1 & 2  & 3   \\
						0 & -1 & 1.5 \\
						0 & 0  & 1   \\
					\end{pmatrix}
					\begin{pmatrix}
						x_1 \\
						x_2 \\
						x_3 \\
					\end{pmatrix}
				&=
					\begin{pmatrix}
						5   \\
						4.5 \\
						-1  \\
					\end{pmatrix}
			\end{align*}
			Therefore,
			\begin{align*}
				x_3                 & = -1  \\
				-x^2 + 1.5 x_3      & = 4.5 \\
				x_1 + 2 x_2 + 3 x_3 & = 5
			\end{align*}
			Therefore, solving,
			\begin{align*}
				x_1 & = 20 \\
				x_2 & = -6 \\
				x_3 & = -1
			\end{align*}
	\end{enumerate}
\end{solution}

\recitation
\section{Condition Number}

\begin{definition}[Condition number]
	The condition number of a matrix $A$, with respect to a particular norm is defined as
	\begin{align*}
		\cond(A) &= \|A\| \cdot \|A^{-1}\|
	\end{align*}
\end{definition}

\begin{theorem}
	Let
	\begin{align*}
		A x &= B
	\end{align*}
	be a matrix equation.\\
	Then,
	\begin{equation*}
		\frac{1}{\cond(A)} \frac{\|e_b\|}{\|b\|} \le \frac{\|e_x\|}{\|x\|} \le \cond(A) \frac{\|e_b\|}{\|b\|}
	\end{equation*}
	and the inequality is tight, i.e. there exist $\overline{x}$, $\overline{e_x}$, $\overline{b}$, $\overline{e_b}$, such that there is an equality, i.e. the bounds are the best bounds possible.
\end{theorem}

\begin{question}
	Consider the system
	\begin{align*}
		A x & = b
	\end{align*}
	where
	\begin{align*}
		A &=
			\begin{pmatrix}
				100 & 99 \\
				99  & 98 \\
			\end{pmatrix}\\
		B &=
			\begin{pmatrix}
				1.005 \\
				0.995 \\
			\end{pmatrix}
	\end{align*}
	The accurate solution is
	\begin{align*}
		x &=
			\begin{pmatrix}
				0.015  \\
				-0.005 \\
			\end{pmatrix}
	\end{align*}
	Consider two approximations of the solution
	\begin{align*}
		\tilde{x}_1 &=
			\begin{pmatrix}
				-0.182 \\
				0.194  \\
			\end{pmatrix}\\
		\tilde{x}_2 &=
			\begin{pmatrix}
				-19.685 \\
				19.895  \\
			\end{pmatrix}
	\end{align*}
	\begin{enumerate}
		\item Find the absolute error and the relative error in the RHS, in the infinity norm.
		\item Fins the relative error, in the infinity norm, in $x$, assuming that $x$ is known.
		\item Can we conclude that a small relative error in the RHS implies a small relaive error in the LHS?
		\item How can this problem be determined without knowing the actual values of $x$?
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}[leftmargin=*]
		\item
			\begin{align*}
				A \tilde{x}_1 &=
					\begin{pmatrix}
						100 & 99 \\
						99  & 98 \\
					\end{pmatrix}
					\begin{pmatrix}
						-0.182 \\
						0.194  \\
					\end{pmatrix}\\
				&=
					\begin{pmatrix}
						1.006 \\
						0.994 \\
					\end{pmatrix}\\
				&= \tilde{b}_1
			\end{align*}
			Therefore,
			\begin{align*}
				e_{b_1} &= b - \tilde{b}_1\\
				&=
					\begin{pmatrix}
						-0.001 \\
						0.001  \\
					\end{pmatrix}
			\end{align*}
			Therefore,
			\begin{align*}
				\|e_{b_1}\|_{\infty} & = 0.001
			\end{align*}
			Therefore,
			\begin{align*}
				\delta_{b_1} & = \frac{\|e_{b_1}\|_{\infty}}{\|b\|_{\infty}} \\
                                             & = \frac{0.001}{1.005}                         \\
                                             & \approx 10^{-3}
			\end{align*}
			Similarly,
			\begin{align*}
				\|e_{b_2}\|_{\infty} & = 0.1 \\
				\delta_{b_2}         & \approx 10^{-1}
			\end{align*}
		\item
			\begin{align*}
				e_1 &= x - \tilde{x}_1\\
				&=
					\begin{pmatrix}
						0.197  \\
						-0.199 \\
					\end{pmatrix}
			\end{align*}
			Therefore,
			\begin{align*}
				\delta_{x_1} & = \frac{\|e_1\|_{\infty}}{\|x\|_{\infty}} \\
                                             & = \frac{0.199}{0.015}                     \\
                                             & \approx 13
			\end{align*}
			Similarly,
			\begin{align*}
				\delta_{x_2} & = 1326
			\end{align*}
		\item
			Therefore, even though the relative error in $B$, i.e. the RHS is small, the error in $x$, i.e. in the LHS is huge.
			Hence, a small relative error in the RHS does not imply a small relative error in the LHS.
		\item
			\begin{align*}
				A &=
					\begin{pmatrix}
						100 & 99 \\
						99  & 98 \\
					\end{pmatrix}\\
				\therefore A^{-1} &=
					\begin{pmatrix}
						-98 & 99   \\
						99  & -100 \\
					\end{pmatrix}
			\end{align*}
			Therefore,
			\begin{align*}
				\|A\|_{\infty}                   & = 199 \\
				\left\| A^{-1} \right\|_{\infty} & = 199
			\end{align*}
			Therefore,
			\begin{align*}
				\cond(A) & = 199^2
			\end{align*}
			Therefore,
			\begin{align*}
				\frac{\|e_x\|}{\|x\|} & \le 199^2 \frac{\|e_b\|}{\|b\|}
			\end{align*}
			and the inequality is tight.\\
			Therefore, as the inequality is tight, it is possible that an error in the RHS can be multiplied by $199^2$ in the LHS.\\
			Therefore if the condition number is large, then such a problem might exist.
			\marginnote
			{
				Realistically, if $A^{-1}$ can be calculated, $x$ can be calculated $x$ accurately.
			}
	\end{enumerate}
\end{solution}

\recitation
\section{Iterative Methods for Systems of Linear Equations}

\begin{algorithm}[H]
	\caption{Jacobi Method}
	\begin{algorithmic}[1]
		\State{Find lower triangular $L$, diagonal $D$, and upper triangular $U$, such that $A = L + D + U$}
		\State{$C \gets D^{-1}$}
		\State{$B_J \gets \left( I - D^{-1} A \right) = -D^{-1} (L + U)$}
		\State{$d_J \gets D^{-1} b$}
		\State{$x^{(n + 1)} \gets B x^{(n)} + d$}
	\end{algorithmic}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Gauss-Seidel Method}
	\begin{algorithmic}[1]
		\State{Find lower triangular $L$, diagonal $D$, and upper triangular $U$, such that $A = L + D + U$}
		\State{$C \gets (L + D)^{-1}$}
		\State{$B_{GS} \gets \left( I - (L + D)^{-1} A \right) = -(L + D)^{-1} U$}
		\State{$d_{GS} \gets (L + D)^{-1} b$}
		\State{$x^{(n + 1)} \gets B x^{(n)} + d$}
	\end{algorithmic}
\end{algorithm}

\begin{theorem}[Sufficient condition for convergence of iterative method for systems of linear equations]
	Let $\|\cdot\|$ be a norm.
	If $\|B\| < 1$, then the method
	\begin{align*}
		x^{(n + 1)} & = B x^{(n)} + d
	\end{align*}
	converges for any initial condition $x^{(0)}$.
\end{theorem}

\begin{theorem}[Necessary condition for convergence of iterative method for systems of linear equations]
	The method
	\begin{align*}
		x^{(n + 1)} & = B x^{(n)} + d
	\end{align*}
	converges for any $x^{(0)}$, if and only if
	\begin{align*}
		\rho(B) & < 1
	\end{align*}
\end{theorem}

\begin{question}
	Let
	\begin{align*}
		A &=
			\begin{pmatrix}
				a & b \\
				c & d \\
			\end{pmatrix}
	\end{align*}
	Consider the system
	\begin{align*}
		A x & = \alpha
	\end{align*}
	\begin{enumerate}
		\item
			Write $B$ for Jacobi and Gauss-Seidel methods.
		\item
			Find, for each method, a sufficient condition for convergence based on the $\infty$ norm.
		\item
			Find, for each method, an equivalent condition for convergence.
		\item
			Does the Jacobi method converge if and only if the Gauss-Seidel method converges?
		\item
			Find a matrix $A$ such that the above equivalent condition is met, but the above sufficient condition is not.
		\item
			Let
			\begin{align*}
				A &=
					\begin{pmatrix}
						2 & 1 \\
						1 & 2 \\
					\end{pmatrix}
			\end{align*}
			and
			\begin{align*}
				\|B_J\|_{\infty}    & = \frac{1}{2} \\
				\|B_{GS}\|_{\infty} & = \frac{1}{2}
			\end{align*}
			If
			\begin{align*}
				x^{(0)} &=
					\begin{pmatrix}
						0 \\
						0 \\
					\end{pmatrix}
			\end{align*}
			then how many iterations are required in order to guarantee a relative error $\frac{\left\| e^{(k)} \right\|_{\infty}}{\|x\|_{\infty}}$ less than $10^{-6}$?
	\end{enumerate}
\end{question}

\begin{solution}
	\begin{enumerate}[leftmargin=*]
		\item
			\begin{align*}
				B_J &= -D^{-1} (L + U)\\
				&= -
					\begin{pmatrix}
						a & 0 \\
						0 & d \\
					\end{pmatrix}^{-1}
					\begin{pmatrix}
						0 & b \\
						c & 0 \\
					\end{pmatrix}\\
				&=
					\begin{pmatrix}
						-\frac{1}{a} & 0            \\
						0            & -\frac{1}{d} \\
					\end{pmatrix}
					\begin{pmatrix}
						0 & b \\
						c & 0 \\
					\end{pmatrix}\\
				&=
					\begin{pmatrix}
						0            & -\frac{b}{a} \\
						-\frac{c}{d} & 0            \\
					\end{pmatrix}
			\end{align*}
			\begin{align*}
				B_{GS} &= -(L + D)^{-1} U\\
				&= -
					\begin{pmatrix}
						a & 0 \\
						c & d \\
					\end{pmatrix}^{-1}
					\begin{pmatrix}
						0 & b \\
						0 & 0 \\
					\end{pmatrix}\\
				&=
					\begin{pmatrix}
						-\frac{1}{a}  & 0            \\
						\frac{c}{a d} & -\frac{1}{d} \\
					\end{pmatrix}
					\begin{pmatrix}
						0 & b \\
						0 & 0 \\
					\end{pmatrix}\\
				&=
					\begin{pmatrix}
						0 & -\frac{b}{a}    \\
						0 & \frac{b c}{a d} \\
					\end{pmatrix}
			\end{align*}
		\item
			A sufficient condition for convergence, for the Jacobi method is
			\begin{align*}
				\|B_J\|_{\infty}                                                                        & < 1 \\
				\therefore \max\left\{ \left| \frac{b}{a} \right| , \left| \frac{c}{d} \right| \right\} & < 1
			\end{align*}
			A sufficient condition for convergence, for the Gauss-Seidel method is
			\begin{align*}
				\|B_{GS}\|_{\infty}                                                                         & < 1 \\
				\therefore \max\left\{ \left| \frac{b}{a} \right| , \left| \frac{b c}{a d} \right| \right\} & < 1
			\end{align*}
		\item
			An equivalent condition for convergence, for the Jacobi method is
			\begin{align*}
				\rho(B_J)                                        & < 1 \\
				\therefore \max\{\lambda_1\}                     & < 1 \\
				\therefore \sqrt{\left| \frac{b c}{a d} \right|} & < 1
			\end{align*}
			Similarly,
			\begin{align*}
				\rho(B_{GS})                              & < 1 \\
				\therefore \left| \frac{b c}{a d} \right| & < 1
			\end{align*}
		\item
			\begin{align*}
				\rho(B_j)                                  & < 1 \\
				\iff \sqrt{\left| \frac{b c}{a d} \right|} & < 1 \\
				\iff \left| \frac{b c}{a d} \right|        & < 1 \\
				\iff \rho(B_{GS})                          & < 1
			\end{align*}
			Therefore, the Jacobi method converges if and only if the Gauss-Seidel method converges.
		\item
			For the above equivalent condition to be met,
			\begin{align*}
				\left| \frac{b c}{a d} \right| & < 1 \\
				\iff |b c|                     & < |a d|
			\end{align*}
			For the above sufficient condition to be not met,
			\begin{align*}
				\left| \frac{b}{a} \right| & > 1 \\
				\iff b > a
			\end{align*}
			Therefore, if
			\begin{align*}
				a & = 1           \\
				b & = 2           \\
				c & = \frac{1}{4} \\
				d & = 1
			\end{align*}
			the matrix $A$ converges.
		\item
			\begin{align*}
				e^{(k)} & = x - x^{(k)} \\
                                        & = B^k e^{(0)}
			\end{align*}
			Therefore,
			\begin{align*}
				\left\| e^{(k)} \right\| & = \left\| B^k e^{(0)} \right\|                    \\
                                                         & \le \left\| B^k \right\| \left\| e^{(0)} \right\| \\
                                                         & \le \|B\|^k \|x - 0\|
			\end{align*}
			Therefore,
			\begin{align*}
				\frac{\left\| e^{(k)} \right\|_{\infty}}{\|x\|_{\infty}} & \le {\|B\|_{\infty}}^k \\
                                                                                         & = 2^{-k}
			\end{align*}
			Therefore, for the required accuracy,
			\begin{align*}
				{\|B\|_{\infty}}^k & < 10^{-6} \\
				\therefore 2^{-k}  & < 10^{-6}
			\end{align*}
			Therefore, $k = 20$ is sufficient for the required accuracy.
	\end{enumerate}
\end{solution}

\recitation
\section{Numerical Differentiation}

\begin{question}
	Let $f \in C^4$, with samples of $f$ at $-h$, $2 h$, and a sample of $f'$ at $-h$ given.
	\begin{enumerate}
		\item Calculate Hermite's interpolation polynomial and derive the formula for the error.
		\item Find an approximation of $f'(h)$ by differentiating the sum of the interpolation polynomial and the error.
	\end{enumerate}<++>
\end{question}<++>

\begin{solution}
	\begin{enumerate}[leftmargin=*]
		\item
			Let
			\begin{align*}
				x_0 & = -h \\
				x_1 & = -h \\
				x_2 & = 2 h
			\end{align*}
			Therefore,
			\begin{align*}
				f[x_0] & = f(-h) \\
				f[x_1] & = f(-h) \\
				f[x_2] & = f(2 h)
			\end{align*}
			Therefore,
			\begin{align*}
				f[x_0,x_1] & = f'(-h) \\
				f[x_1,x_2] & = \frac{f(2 h) - f(-h)}{3 h}
			\end{align*}
			Therefore,
			\begin{align*}
				f[x_0,x_1,x_2] & = \frac{f(2 h) - f(-h) - 3 h f'(-h)}{9 h^2}
			\end{align*}
			Therefore,
			\begin{align*}
				p_2(x) & = f[x_0] + f[x_0,x_1] (x - x_0)^2 + f[x_0,x_1,x_2] (x - x_0) (x - x_2) \\
                                       & = f(-h) + f'(-h) (x + h) + \frac{f(2 h) - f(-h) - 3 h f'(-h)}{9 h^2} (x + h)^2
			\end{align*}
			Therefore,
			\begin{align*}
				\psi(x) & = \prod (x - x_i) \\
                                        & = (x + h)^2 (x - 2 h)
			\end{align*}
			Therefore,
			\begin{align*}
				f(x) & = p_2(x) + f[-h,-h,2 h,x] \psi(x) \\
                                     & = p_2(x) f[-h,-h,2 h,x] (x + h)^2 (x - 2 h)
			\end{align*}
			Therefore, the error is
			\begin{align*}
				e(x) & = f[-h,-h,2 h,x] (x + h)^2 (x - 2 h)
			\end{align*}
		\item
			\begin{align*}
				f'(h)                & \approx {p_2}'(h)                                              \\
				\therefore {p_2}'(x) & = f'(-h) + 2 \frac{f(2 h) - f(-h) - 3 h f'(-h)}{9 h^2} (x + h) \\
				\therefore {p_2}'(h) & = f'(-h) + 4 \frac{f(2 h) - f(-h) - 3 h f(-h)}{9 h}            \\
				\therefore f'(h)     & \approx f'(-h) + 4 \frac{f(2 h) - f(-h) - 3 h f(-h)}{9 h}
			\end{align*}
			The error is
			\begin{align*}
				f(x)             & = p_2(x) + e(x)     \\
				\therefore f'(x) & = {p_2}'(x) + e'(x) \\
				\therefore f'(h) & = {p_2}'(h) + e'(h)
			\end{align*}
			Therefore,
			\begin{align*}
				e(x)             & = f[-h,-h,2 h] \psi(x)                               \\
				\therefore e'(x) & = f[-h,-h,2 h,x,x] \psi(x) + f[-h,-h,2 h,x] \psi'(x) \\
				\therefore e'(h) & = f[-h,-h,2 h,h,h] \psi(h) + f[-h,-h,2 h,h] \psi'(h)
			\end{align*}
			Therefore, substituting $\psi(h)$ and $\psi'(h)$, for $c \in [-h,2 h]$,
			\begin{align*}
				e'(h)  & = f[-h,-h,2 h,h,h] \left( -4 h^3 \right)
				\marginnote
				{
					$f[x_0,\dots,x_n] = \frac{f^{(n)}(c)}{n!}$
				}     \\
                                       & = -\frac{f^{(4)}(c)}{4!} 4 h^3 \\
                                       & = \mathrm{O}\left( h^3 \right)
			\end{align*}
	\end{enumerate}
\end{solution}

\end{document}
