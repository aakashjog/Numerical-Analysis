\documentclass[fleqn, a4paper, 12pt, twoside]{article}
\usepackage{exsheets} %question and solution environments
\usepackage{amsmath, amssymb, amsthm} %standard AMS packages
\usepackage{esint} %integral signs
\usepackage{marginnote} %marginnotes
\usepackage{gensymb} %miscellaneous symbols
\usepackage{commath} %differential symbols
\usepackage{xcolor} %colours
\usepackage{cancel} %cancelling terms
\usepackage{siunitx} %formatting units
\usepackage{tikz, pgfplots} %diagrams
	\usetikzlibrary{calc, hobby, patterns, intersections, angles, quotes, spy}
\usepackage{graphicx} %inserting graphics
\usepackage{epstopdf} %converting and inserting eps graphics
\usepackage{hyperref} %hyperlinks
\usepackage{datetime} %date and time
\usepackage{ulem} %underline for \emph{}
\usepackage{xfrac, lmodern} %inline fractions
\usepackage{enumerate, enumitem} %numbered lists
\usepackage{float} %inserting floats
\usepackage[american voltages]{circuitikz} %circuit diagrams
\usepackage{pdflscape} %pages in landscape orientation
\usepackage{setspace} %double spacing
\usepackage{microtype} %micro-typography
\usepackage{listings} %formatting code
	\lstset{language=Matlab}
	\lstdefinestyle{standardMatlab}
	{
		belowcaptionskip=1\baselineskip,
		breaklines=true,
		frame=L,
		xleftmargin=\parindent,
		language=C,
		showstringspaces=false,
		basicstyle=\footnotesize\ttfamily,
		keywordstyle=\bfseries\color{green!40!black},
		commentstyle=\itshape\color{purple!40!black},
		identifierstyle=\color{blue},
		stringstyle=\color{orange},
	}
\usepackage{algpseudocode} %algorithms
\usepackage{algorithm} %algorithms

\newcommand\numberthis{\addtocounter{equation}{1}\tag{\theequation}} %adds numbers to specific equations in non-numbered list of equations

\theoremstyle{definition}
\newtheorem{example}{Example}
\newtheorem{definition}{Definition}

\theoremstyle{theorem}
\newtheorem{theorem}{Theorem}
\newtheorem{law}{Law}

\newcommand{\curl}{\mathrm{curl\,}}

\newcommand{\divergence}{\mathrm{div\,}}

\makeatletter
\@addtoreset{section}{part} %resets section numbers in new part
\makeatother

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}

\renewcommand{\tilde}{\widetilde}

\renewcommand{\marginfont}{\scriptsize \color{blue}}

\SetupExSheets{solution/print = true} %prints all solutions by default

%opening
\title{Numerical Analysis}
\author{Aakash Jog}
\date{2015-16}

\begin{document}

\maketitle
%\setlength{\mathindent}{0pt}

\blfootnote
{	
	\begin{figure}[H]
		\includegraphics[height = 12pt]{cc.eps}
		\includegraphics[height = 12pt]{by.eps}
		\includegraphics[height = 12pt]{nc.eps}
		\includegraphics[height = 12pt]{sa.eps}
	\end{figure}
	This work is licensed under the Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. To view a copy of this license, visit \url{http://creativecommons.org/licenses/by-nc-sa/4.0/}.
} %CC-BY-NC-SA license

\tableofcontents

\newpage
\section{Lecturer Information}

\textbf{Prof. Nir Sochen}\\
~\\
Office: Schreiber 201\\
Telephone: \href{tel:+972 3-640-8044}{+972 3-640-8044}\\
E-mail: \href{mailto:sochen@post.tau.ac.il}{sochen@post.tau.ac.il}\\
Office Hours: Sundays, 10:00--12:00

\section{Required Reading}

\begin{enumerate}
	\item S. D. Conte and C. de Boor, Elementary Numerical Analysis, 1972
\end{enumerate}

\newpage

\section{Floating Point Representation}

\begin{question}
	Represent 9.75 in base 2.
\end{question}

\begin{solution}
	\begin{align*}
		9.75 & = 8 + 1 + \frac{1}{2} + \frac{1}{4}                                   \\
                     & = 2^3 + 2^0 + 2^{-1} + 2^{-2}                                         \\
                     & = 2^3 \left( 2^0 + 2^{-3} + 2^{-4} + 2^{-5} \right)                   \\
                     & = \left( 2^{11} \left( 1 + 0.001 + 0.0001 + 0.00001 \right) \right)_2 \\
                     & = \left( 2^{11} \left( 1.00111 \right) \right)_2
	\end{align*}
\end{solution}

\begin{definition}[Double precision floating point representation]
	A floating point representation which uses 64 bits for representation of a number is called a double precision floating point representation.\\
	The standard form of double precision representation is
	\begin{align*}
		a & = \underbrace{\pm}_{\text{1 bit}} \underbrace{1}_{\text{1 bit}}.\underbrace{\cdots}_{\text{52 bits}} \times w^{\underbrace{\pm}_{\text{1 bit}} \underbrace{\cdots}_{\text{10 bits}}}
	\end{align*}
\end{definition}

\begin{theorem}[Range of double precision floating point representation]
	The largest number which can be represented with double precision floating point representation is approximately $10^{307}$ and the smallest number which can be represented is approximately $10^{-307}$.
	\label{Range_of_double_precision_floating_point_representation}
\end{theorem}

\begin{proof}
	As the exponent has 10 bits for representation,
	\begin{equation*}
		-\left( 10^{10} - 1 \right) \le \textnormal{exponent} \le \left( 10^{10} - 1 \right)
	\end{equation*}
	Therefore,
	\begin{equation*}
		-1023 \le \textnormal{exponent} \le 1023
	\end{equation*}
	Therefore, the smallest number, in terms of absolute value, which can be represented, is
	\begin{align*}
		1.\underbrace{0 \cdots 0}_{\text{52 bits}} \times 2^{-1024} \approx 10^{-307}
	\end{align*}
	Therefore, the smallest number which can be represented is approximately $10^{-307}$, and the largest number which can be represented is approximately $10^{307}$.
\end{proof}

\begin{definition}[Overflow]
	If a result is larger than the largest number which can be represented, it is called overflow.
\end{definition}

\begin{definition}[Underflow]
	If a result is smaller than the smallest number which can be represented, it is called underflow.
\end{definition}

\begin{definition}[Least significant digit]
	\begin{align*}
		1 & = 1.\underbrace{0 \cdots 0}_{\text{52 zeros}} \times 2^0 \\
	\end{align*}
	Let $1_{\varepsilon}$ be the smallest number larger than 1, which can be represented in double precision floating point representation.\\
	Therefore,
	\begin{align*}
		1 & = 1.\underbrace{0 \cdots 0}_{\text{51 zeros}} 1 \times 2^0 \\
                  & = 1 + 2^{-52}                                              \\
                  & \approx 1 + 2 \times 10^{-16}
	\end{align*}
	Therefore,
	\begin{align*}
		1 - 1_{\varepsilon} & = 2^{-52} \\
                                    & \approx 2 \times 10^{-16}
	\end{align*}
	This number is called the least significant digit, or the machine precision.
	It is the maximum possible error in representation.
	It is represented by $\varepsilon$.
	\label{LSD}
\end{definition}

\begin{definition}[Error]
	Let the DPFP representation of a number $x$ be $\tilde{x}$.\\
	The absolute error in representation is defined as
	\begin{align*}
		\textnormal{absolute error} & = \left| x - \tilde{x} \right| \\
                                            & = 0.0 \cdots 01 \times 2^{\text{exponent}}
	\end{align*}
	The relative error in representation is defined as
	\begin{align*}
		\delta & = \frac{\left| x - \tilde{x} \right|}{x} \\
                       & = 0.0 \cdots 01                          \\
                       & < \varepsilon
	\end{align*}
	The maximum error, $2^{-52} \approx 2 \times 10^{-16}$, is called the machine precision.\\
	In general,
	\begin{align*}
		\tilde{x} \, \tilde{\star} \, \tilde{y} & = \left( x \star y \right) \left( 1 + \delta \right)
	\end{align*}
	where $\delta$ is the relative error, $\varepsilon$ is the machine precision, $\delta < \varepsilon$, and $\star$ is an operator. 
\end{definition}

\subsection{Loss of Significant Digits in Addition and Subtraction}

\begin{question}
	Represent $\pi + \frac{1}{30}$ in base 10 with 4 digits.
\end{question}

\begin{solution}
	\begin{align*}
		\pi & \approx 3.14159
	\end{align*}
	Approximating by ignoring the last digits,
	\begin{align*}
		\tilde{\pi} & = 3.141
	\end{align*}
	Similarly,
	\begin{align*}
		\tilde{\frac{1}{30}} & = 3.333 \times 10^{-2} \\
	\end{align*}
	Therefore, adding,
	\begin{align*}
		\tilde{\pi} + \tilde{\frac{1}{30}} & = 3.141 + 0.03333 \\
                                                   & = 3.174
	\end{align*}
	Therefore,
	\begin{align*}
		\delta & = \left| \frac{\left( \tilde{\pi} + \tilde{\frac{1}{30}} \right) - \left( \pi + \frac{1}{30} \right)}{\pi + \frac{1}{30}} \right| \\
                       & = 0.0003
	\end{align*}
	Therefore, $\delta < \varepsilon = 0.001$
\end{solution}

\begin{question}
	Given
	\begin{align*}
		a & = 1.435234 \\
		b & = 1.429111
	\end{align*}
	Find the relative error.
\end{question}

\begin{solution}
	\begin{align*}
		a & = 1.435234 \\
		b & = 1.429111
	\end{align*}
	Therefore,
	\begin{align*}
		a - b & = 0.0061234
	\end{align*}
	Approximating by ignoring the last digits,
	\begin{align*}
		\tilde{a} & = 1.435 \\
		\tilde{b} & = 1.429
	\end{align*}
	Therefore,
	\begin{align*}
		\tilde{a} - \tilde{b} & = 0.006
	\end{align*}
	Therefore,
	\begin{align*}
		\delta & = \left| \frac{\left( a - b \right) - \left( \tilde{a} - \tilde{b} \right)}{a - b} \right|
	\end{align*}
	Therefore,
	\begin{align*}
		\delta > 10^{-3} \\
		\therefore \delta > \varepsilon
	\end{align*}
\end{solution}

\begin{question}
	Solve
	\begin{align*}
		x^2 + 10^8 x + 1 & = 0
	\end{align*}
\end{question}

\begin{solution}
	\begin{align*}
		x & = \frac{-10^8 \pm \sqrt{10^{16} - 4}}{2}
	\end{align*}
	Therefore,
	\begin{align*}
		x_{-} & \approx -10^8
	\end{align*}
	Therefore, by Vietta Rules,
	\begin{align*}
		x_1 x_2   & = \frac{c}{a}  \\
		x_1 + x_2 & = -\frac{b}{a} \\
	\end{align*}
	Therefore,
	\begin{align*}
		x_{+} x_{-}      & = 1               \\
		\therefore x_{+} & = \frac{1}{x_{-}} \\
                                 & \approx -10^{-8}
	\end{align*}
	In MATLAB, this can be executed as \lstinline!x = roots([1,10^8,1])!\\
	This gives the result
	\begin{align*}
		x_{+} & = -7.45 \times 10^{-9}
	\end{align*}
	Therefore, the absolute error is
	\begin{align*}
		\left| \tilde{x} - x \right| & = \left| -7.45 \times 10^{-9} - \left( -10^{-8} \right) \right| \\
                                             & = 2.55 \times 10^{-9}
	\end{align*}
	Therefore,
	\begin{align*}
		\delta & = \left| \frac{\tilde{x} - x}{x} \right|             \\
                       & = \left| \frac{2.55 \times 10^{-9}}{10^{-8}} \right| \\
                       & = 0.255                                              \\
                       & = 25 \%
	\end{align*}
	The algorithm used by MATLAB is
	\begin{algorithmic}
		\If{$b \ge 0$}
			\State $x_1 = \frac{-b - \sqrt{b^2 - 4 a c}}{2 a}$
			\State $x_2 = \frac{x}{a x_1}$
		\Else
			\State $x_2 = \frac{-b + \sqrt{b^2 - 4 a c}}{2 a}$
			\State $x_1 = \frac{c}{a x_2}$
		\EndIf
	\end{algorithmic}
	This is done to avoid subtraction of numbers close to each other, and hence avoid the possible error.
\end{solution}

\section{Series of Approximations}

\subsection{Order of Convergence}

\begin{definition}
	Let $\{\alpha_n\}_{n = 1}^{\infty}$ be a series.
	$\{\alpha_n\}$ is said to converge to $\alpha$, denoted as $\alpha_n \to \alpha$, if $\forall \varepsilon > 0$, $\varepsilon \in \mathbb{R}$, $\exists n_0(\varepsilon) \in \mathbb{N}$, such that $\forall n \in \mathbb{N}$, $n > n_0(\varepsilon)$, $|\alpha_n - \alpha| < \varepsilon$.
\end{definition}

Usually, the series $\{\alpha_n\}$ is compared to a simpler series such as $\frac{1}{n} , \frac{1}{n^{\beta}} , \dots$.

\begin{definition}
	$\alpha_n$ is said to be ``big-O'' of $\beta_n$, and is said to behave like $\beta_n$, if $\exists k \in \mathbb{R}$, $k > 0$, $\exists n_0 \in \mathbb{N}$, $n_0 > 0$, such that $\forall n > n_0$,
	\begin{align*}
		|\alpha_n| &\le k |\beta_n|
	\end{align*}
	It is denoted as
	\begin{align*}
		\alpha_n &= \mathrm{O}(\beta_n)
	\end{align*}
\end{definition}

\begin{definition}
	$\alpha_n$ is said to be ``small-O'' of $\beta_n$ if
	\begin{align*}
		\lim\limits_{n \to \infty} \frac{\alpha_n}{\beta_n} &= 0
	\end{align*}
	It is denoted as
	\begin{align*}
		\alpha_n &= \mathrm{o}(\beta_n)
	\end{align*}
\end{definition}

\begin{question}
	Find the order of convergence of
	\begin{align*}
		\alpha_n &= 2 n^3 + 3 n^2 + 4 n + 5
	\end{align*}
\end{question}

\begin{solution}
	\begin{align*}
		\alpha_n &= 2 n^3 + 3 n^2 + 4 n + 5\\
		&\le (2 + 3 + 4 + 5) n^3\\
		\therefore \alpha_n &\le 14 n^3
	\end{align*}
	Therefore, comparing to the standard form,
	\begin{align*}
		k &= 14\\
		\beta_n &= n^3
	\end{align*}
	Therefore, as $\forall n \ge 1$, $|a_n| \le 14 |\beta_n|$,
	\begin{align*}
		\alpha_n &= \mathrm{O}(\beta_n)
	\end{align*}
	Also,
	\begin{align*}
		\lim\limits_{n \to \infty} \frac{\alpha_n}{\beta_n} &= \lim\limits_{n \to \infty} \frac{2 n^3 + 2 n^2 + 4 n + 5}{n^3}\\
		&= 2
	\end{align*}
	Therefore, as the limits is not zero,
	\begin{align*}
		\alpha_n &\neq \mathrm{o}(\beta_n)
	\end{align*}
	However, $\forall \delta > 0$,
	\begin{align*}
		\alpha_n &= \mathrm{o}\left( n^{3 + \delta} \right)
	\end{align*}
\end{solution}

\section{Representation of Polynomials}

\subsection{Power series}	

\begin{definition}[Power series representation of polynomials]
	\begin{align*}
		P_n(x) &= a_0 + a_1 x + \dots + a_n x^n
	\end{align*}
\end{definition}

This representation may lead to loss of significant digits.

\begin{question}
	Let $P(x)$ represent a straight line.
	\begin{align*}
		P(6000) &= \frac{1}{3}\\
		P(6001) &= -\frac{2}{3}
	\end{align*}
	If only 5 decimal digits are used, show that there is a loss of significant digits, if the power series representation of the polynomial is used.
\end{question}

\begin{solution}
	$P(x)$ represents a straight line.
	Therefore,
	\begin{align*}
		P(x) &= a x + b
	\end{align*}
	Therefore,
	\begin{align*}
		6000 a + b &= \frac{1}{3}\\
		6001 a + b &= -\frac{2}{3}
	\end{align*}
	Therefore,
	\begin{align*}
			\begin{pmatrix}
				6000 & 1\\
				6001 & 1\\
			\end{pmatrix}
			\begin{pmatrix}
				a\\
				b\\
			\end{pmatrix}
		&=
			\begin{pmatrix}
				\frac{1}{3}\\
				-\frac{2}{3}\\
			\end{pmatrix}\\
		\therefore
			\begin{pmatrix}
				a\\
				b\\
			\end{pmatrix}\\
		&=
			\frac{1}{|A|}
			\begin{pmatrix}
				1 & -1\\
				-6001 & 6000
			\end{pmatrix}
			\begin{pmatrix}
				\frac{1}{3}\\
				-\frac{2}{3}\\
			\end{pmatrix}\\
		&=
			-
			\begin{pmatrix}
				1\\
				-6000.3\\
			\end{pmatrix}\\
		&=
			\begin{pmatrix}
				-1\\
				6000.3\\
			\end{pmatrix}
	\end{align*}
	Therefore,
	\begin{align*}
		a &= -1\\
		b &= 6000.3
	\end{align*}
	Therefore,
	\begin{align*}
		P(x) &= -x + 6000.3
	\end{align*}
	Substituting $6000$ and $6001$ in this expression,
	\begin{align*}
		P(6000) &= 0.3\\
		P(6001) &= 0.7
	\end{align*}
	However, the most accurate values of $P(6000)$ and $P(6001)$, using 5 decimal digits only, should be
	\begin{align*}
		P(6000) &= 0.33333\\
		P(6001) &= -0.66666
	\end{align*}
	Therefore, there is a loss of significant digits.
\end{solution}

\subsection{Shifted Power Series}

\begin{definition}[Shifted power series representation of polynomials]
	\begin{align*}
		P_n(x) &= a_0 + a_1 (x - c) + \dots + a_n (x - c)^n
	\end{align*}
\end{definition}

This representation is a power series shifted by $c$.
Hence, this representation does not lead to loss of significant digits.

\begin{question}
	Let $P(x)$ be a straight line.
	\begin{align*}
		P(6000) &= \frac{1}{3}\\
		P(6001) &= -\frac{2}{3}
	\end{align*}
	If only 5 decimal digits are used, show that there is no loss of significant digits, if the shifted power series representation of the polynomial is used, with $c = 6000$.
\end{question}

\begin{solution}
	$P(x)$ represents a straight line.
	Therefore,
	\begin{align*}
		P(x) &= a (x - 6000) + b
	\end{align*}
	Therefore,
	\begin{align*}
		b &= \frac{1}{3}\\
		a + b &= -0.66666\\
		\therefore a &= -0.99999
	\end{align*}
	Therefore,
	\begin{align*}
		P(x) &= -0.99999 (x - 6000) + 0.33333
	\end{align*}
	Substituting $6000$ and $6001$ in this expression,
	\begin{align*}
		P(6000) &= 0.33333\\
		P(6001) &= -0.66666
	\end{align*}
	Therefore, there is no loss of significant digits, as the values of $P(6000)$ and $P(6001)$ are the most accurate values possible, using 5 decimal digits.
\end{solution}

\subsection{Newton's Form}

\begin{definition}[Newton's form of representation of polynomials]
	\begin{align*}
		P_n(x) &= a_0 + a_1 (x - c_1) + \dots + a_n (x - c_1) \dots (x - c_n)
	\end{align*}
\end{definition}

The number of multiplications needed to calculate $P_n(x)$ is 
\begin{align*}
	\sum\limits_{i = 1}^{n} i &= \frac{n (n + 1)}{2}
\end{align*}
The number of additions or subtractions needed to calculate $P_n(x)$ is 
\begin{align*}
	\sum\limits_{i = 1}^{n} i + n &= \frac{n (n + 1)}{2} + n
\end{align*}
Therefore, the total number of operations needed to calculate $P_n(x)$ is $\mathrm{O}(n^2)$.

\subsection{Nested Newton's Form}

\begin{definition}[Nested Newton's form of representation of polynomials]
	\begin{align*}
		P_n(x) &= a_0 + (x - c_1) \left( a_1 + (x - c_2) \left( a_2 + (x - c_3) \left( \dots \right) \right) \right)
	\end{align*}
\end{definition}

The number of multiplications needed to calculate $P_n(x)$ is 
\begin{align*}
	\sum\limits_{i = 1}^{n} 1 &= n
\end{align*}
The number of additions or subtractions needed to calculate $P_n(x)$ is 
\begin{align*}
	\sum\limits_{i = 1}^{n} 2 &= 2 n
\end{align*}
Therefore, the total number of operations needed to calculate $P_n(x)$ is big-O of $\mathrm{O}(n)$.

\subsection{Properties of Polynomials}

\begin{theorem}
	For a polynomial in shifted power series form,
	\begin{align*}
		P_n(x) &= P_n(c) + (x - c) q_{n - 1}(x)
	\end{align*}
\end{theorem}

\begin{proof}
	\begin{align*}
		P_n(x) &= a_0 + a_1 (x - c) + \dots + a_n(x - c)^n\\
		&= a_0 + (x - c) \left( a_1 + a_2 (x - 2) + \dots + a_n (x - c)^{n - 1} \right)\\
		&= a_0 + (x - c) q_{n - 1}(x)\\
		&= P_n(c) + (x - c) q_{n - 1}(x)
	\end{align*}
\end{proof}

\begin{theorem}
	If $c$ is a root of $P_n(x)$, i.e., if
	\begin{align*}
		P_n(c) &= 0
	\end{align*}
	then
	\begin{align*}
		P_n(x) &= (x - c) q_{n - 1}(x)
	\end{align*}
	If $c_1 \neq c_2$ are roots of $P_n(x)$, then
	\begin{align*}
		P_n(x) &= (x - c_1) (x - c_2) r_{n - 2}(x)
	\end{align*}
	Similarly, if $P_n(x)$ has $n$ different roots, then
	\begin{align*}
		P_n(x) &= A (x - c_1) \dots (x - c_n)
	\end{align*}
	where $A \in \mathbb{R}$.\\
	If $P_n(x)$ has $n + 1$ different roots, then
	\begin{align*}
		P_n(x) &= A (x - c_1) \dots (x - c_n) (x - c_{n + 1})
	\end{align*}
	where $A = 0$.
\end{theorem}

\begin{theorem}
	If $p(x)$ and $q(x)$ are polynomials of degree at most $n$, that satisfy
	\begin{align*}
		p(x_i) &= f(x_i)\\
		q(x_i) &= f(x_i)
	\end{align*}
	for $i \in \{0,\dots,n\}$, then
	\begin{align*}
		p_n(x) &\equiv q_n(x)
	\end{align*}
	This means that there exists a unique polynomial with degree $n$ which passes through $n + 1$ points, i.e. $n + 1$ points define a unique $n$ degree polynomial.
\end{theorem}

\begin{proof}
	Let
	\begin{align*}
		d_n(x) &= p_n(x) - q_n(x)
	\end{align*}
	Therefore, $d_n(x)$ is a polynomial of degree at most $n$, which has $n + 1$ roots.
	Therefore,
	\begin{align*}
		d_n(x) \equiv 0
	\end{align*}
	Therefore,
	\begin{align*}
		p_n(x) &\equiv q_n(x)
	\end{align*}
\end{proof}

\section{Interpolation}

\begin{theorem}[Weierstrass Approximation Theorem]
	Let $f(x) \in \mathrm{C} [a,b]$, i.e. it is continuous on $[a,b]$.
	Let $\varepsilon > 0$.
	Then there exists a polynomial $P(x)$ defined on $[a,b]$, such that $\forall x \in [a,b]$,
	\begin{align*}
		\left| f(x) - P(x) \right| &< \varepsilon
	\end{align*}
	\label{Weierstrass_Approximation_Theorem}
\end{theorem}

\begin{definition}[Interpolating polynomial]
	$p(x)$ is said to be the interpolating polynomial of $f(x)$, if for all sample points $x_i$,
	\begin{align*}
		f(x_i) & = p(x_i)
	\end{align*}
\end{definition}

\begin{theorem}
	Let $f(x)$ such that $\forall i \in \{0,\dots,n\}$,
	\begin{align*}
		f(x_i) & = y_i
	\end{align*}
	Then, there exists a unique polynomial $p(x)$ of degree at most $n$, which interpolates $f(x)$ at all sample points $x_i$.
\end{theorem}

\subsection{Direct Method}

\begin{definition}[Van der Monde matrix]
	Let
	\begin{align*}
		p(x) & = \sum\limits_{i = 0}^{n} a_i x^i
	\end{align*}
	Let
	\begin{align*}
		f(x_i) & = y_i
	\end{align*}
	Therefore, as
	\begin{align*}
		p(x_i) & = f(x_i)
	\end{align*}
	the constraints are
	\begin{align*}
		a_0 + a_1 x_0 + \dots + a_n {x_0}^n & = y_0  \\
		a_1 + a_1 x_1 + \dots + a_n {x_1}^n & = y_1  \\
                                                    & \vdots \\
		a_n + a_1 x_n + \dots + a_n {x_n}^n & = y_n  \\
	\end{align*}
	Therefore,
	\begin{align*}
			\begin{pmatrix}
				1      & x_0    & {x_0}^2 & \dots & {x_0}^n \\
				1      & x_1    & {x_1}^2 & \dots & {x_1}^n \\
				1      & x_2    & {x_2}^2 & \dots & {x_2}^n \\
				\vdots & \vdots & \vdots  &       & \vdots  \\
				1      & x_n    & {x_n}^2 & \dots & {x_n}^n \\
			\end{pmatrix}
			\begin{pmatrix}
				a_0    \\
				a_1    \\
				a_2    \\
				\vdots \\
				a_n    \\
			\end{pmatrix}
		&=
			\begin{pmatrix}
				y_0    \\
				y_1    \\
				y_2    \\
				\vdots \\
				y_n    \\
			\end{pmatrix}
	\end{align*}
	The matrix
	\begin{align*}
		V &=
			\begin{pmatrix}
				1      & x_0    & {x_0}^2 & \dots & {x_0}^n \\
				1      & x_1    & {x_1}^2 & \dots & {x_1}^n \\
				1      & x_2    & {x_2}^2 & \dots & {x_2}^n \\
				\vdots & \vdots & \vdots  &       & \vdots  \\
				1      & x_n    & {x_n}^2 & \dots & {x_n}^n \\
			\end{pmatrix}
	\end{align*}
	is called the Van der Monde matrix.
\end{definition}

\begin{theorem}
	The Van der Monde matrix is invertible, and hence there exists a unique matrix of coefficients $a_0 , \dots , a_n$, and hence the interpolating polynomial $p(x)$ is unique.
\end{theorem}

\subsection{Lagrange's Interpolation}

\begin{definition}[Lagrange polynomials]
	Let
	\begin{align*}
		L_k(x) & = \prod\limits_{i = 0 ; i \neq k}^{n} (x - x_i)
	\end{align*}
	Therefore,
	\begin{align*}
		L_k(x_i) &=
			\begin{cases}
				0 & ;\quad i \neq k \\
				1 & ;\quad i = k    \\
			\end{cases}
	\end{align*}
	Let
	\begin{align*}
		l_k(x) & = \frac{L_k(x)}{L_k(x_k)}
	\end{align*}
	Therefore,
	\begin{align*}
		l_k(x_i) &=
			\begin{cases}
				0 & ;\quad i \neq k \\
				1 & ;\quad i = k    \\
			\end{cases}
	\end{align*}
	The polynomials $l_i(x)$ are called Lagrange polynomials.
\end{definition}

\begin{theorem}
	Let
	\begin{align*}
		p_n(x) & = \sum\limits_{i = 0}^{n} f(x_i) l_i(x)
	\end{align*}
	where $l_i(x)$ are Lagrange polynomials.\\
	Then, $p_n(x)$ is the interpolating polynomial of $f(x)$.
\end{theorem}

\begin{question}
	Which polynomial of degree 2 interpolates the below data?
	\begin{table}[H]
		\centering
		\begin{tabular}{c|c}
			$x$ & $f(x)$\\
			\hline
			$1$ & $1$\\
			$2$ & $3$\\
			$3$ & $7$\\
		\end{tabular}
	\end{table}
\end{question}

\begin{solution}
	\begin{align*}
		L_k(x) & = \prod\limits_{i = 0 ; i \neq k}^{n} (x - x_i)
	\end{align*}
	Therefore,
	\begin{align*}
		L_1(x) & = (x - 2) (x - 3) \\
		L_2(x) & = (x - 1) (x - 3) \\
		L_3(x) & = (x - 1) (x - 2)
	\end{align*}
	Therefore,
	\begin{align*}
		L_1(1) & = (1 - 2) (1 - 3) \\
                       & = 2               \\
		L_2(2) & = (2 - 1) (2 - 3) \\
                       & = -1              \\
		L_3(3) & = (3 - 1) (3 - 2) \\
                       & = 2
	\end{align*}
	Therefore,
	\begin{align*}
		l_k(x) & = \frac{L_k(x)}{L_k(x_k)}
	\end{align*}
	Therefore,
	\begin{align*}
		l_1(x) & = \frac{L_1(x)}{L_1(1)}       \\
                       & = \frac{1}{2} (x - 2) (x - 3) \\
		l_2(x) & = \frac{L_2(x)}{L_2(1)}       \\
                       & = -(x - 1) (x - 3)            \\
		l_3(x) & = \frac{L_3(x)}{L_3(1)}       \\
                       & = \frac{1}{2} (x - 1) (x - 2)
	\end{align*}
	Therefore,
	\begin{align*}
		p_2(x) & = \sum f(x_i) l_i(x) \\
                       & = \frac{1}{2} (x - 2) (x - 3) - 3 (x - 1) (x - 3) + \frac{7}{2} (x - 1) (x - 2)
	\end{align*}
\end{solution}

\begin{question}
	Given
	\begin{align*}
		k(z) & = \int\limits_{0}^{\frac{\pi}{2}} \frac{\dif x}{\sqrt{1 - (\sin z)^2 (\sin x)^2}}
	\end{align*}
	and
	\begin{align*}
		k(1) & = 1.5709 \\
		k(4) & = 1.5727 \\
		k(6) & = 1.5751
	\end{align*}
	approximate $k(3.5)$.
\end{question}

\begin{solution}
	\begin{align*}
		l_k(x) & = \frac{\prod\limits_{i = 0 ; i \neq k}^{n} (x - x_i)}{\prod\limits_{i = 0 ; i \neq k}^{n} (x_k - x_i)}
	\end{align*}
	Therefore,
	\begin{align*}
		l_1(x) & = \frac{(x - 4) (x - 6)}{(1 - 4) (1 - 6)} \\
		l_4(x) & = \frac{(x - 1) (x - 6)}{(4 - 1) (4 - 6)} \\
		l_6(x) & = \frac{(x - 1) (x - 4)}{(6 - 1) (6 - 4)}
	\end{align*}
	Therefore,
	\begin{align*}
		l_1(3.5) & = \frac{(3.5 - 4) (3.5 - 6)}{(1 - 4) (1 - 6)} \\
                         & = 0.08333                                     \\
		l_4(3.5) & = \frac{(3.5 - 1) (3.5 - 6)}{(4 - 1) (4 - 6)} \\
                         & = 1.04167                                     \\
		l_6(3.5) & = \frac{(3.5 - 1) (3.5 - 4)}{(6 - 1) (6 - 4)} \\
                         & = -0.125
	\end{align*}
	Therefore,
	\begin{align*}
		p_2(x)              & = \sum f(x_i) l_k(x)                                          \\
		\therefore p_2(3.5) & = \sum f(x_1) l_k(3.5)                                        \\
                                    & = (1.5709) (0.08333) + (1.5727) (1.04167) + (1.5751) (-0.125) \\
                                    & = 1.57225
	\end{align*}
\end{solution}

\subsection{Hermite Polynomials}

\begin{definition}
	Let the given data be of the form $\left( x_i , f(x_i) , f'(x_i) \right)$, where $i = 0,\dots,n$.\\
	$H_{2 n + 1}$ is called the Hermite polynomial of $f(x)$.\\
	For $H_{2 n + 1}$ to be the interpolation polynomial of $f(x)$, the constraints are
	\begin{align*}
		H_{2 n + 1}(x_i)  & = f(x_i) \\
		H_{2 n + 1}'(x_i) & = f'(x_i)
	\end{align*}
	Therefore, the number of constraints are $2 n + 2$.\\
	Hence, the polynomial is of degree at most $2 n + 1$.\\
\end{definition}

\begin{theorem}
	Let
	\begin{align*}
		H_{2 n + 1}(x) & = \sum\limits_{i = 0}^{n} f(x_i) \psi_{n,i}(x) + \sum\limits_{i = 0}^{n} f'(x_i) \varphi_{n,i}(x)
	\end{align*}
	Let
	\begin{align*}
		\delta_{i j} &=
			\begin{cases}
				0 & ;\quad i \neq j \\
				1 & ;\quad i = j    \\
			\end{cases}
	\end{align*}
	If the polynomials $\psi$ and $\varphi$ satisfy
	\begin{align*}
		\psi_{n,i}(x_j)        & = \delta_{i j} \\
		{\psi_{n,i}}'(x_j)     & = 0            \\
		\varphi_{n,i}(x_j)     & = 0            \\
		{\varphi'_{n,i}}'(x_j) & = \delta_{i j}
	\end{align*}
	then the polynomial $H_{2 n + 1}$ is the interpolation polynomial of $f(x)$.
\end{theorem}

\subsection{Newton's Interpolation}

\begin{definition}[Newton's polynomial]
	The polynomial
	\begin{align*}
		p_n(x) & = \sum\limits_{i = 0}^{n} A_i \prod\limits_{j = 0}^{i - 1} (x - x_j)
	\end{align*}
	is called Newton's polynomial.
\end{definition}

\begin{theorem}
	If $p_k(x)$, constructed based on $x_1,\dots,x_k$ is known, then $p_{k + 1}(x)$, based on $x_1,\dots,x_{k + 1}$ can be constructed as
	\begin{align*}
		p_{k + 1}(x) & = p_k(x) + A_{k + 1} (x - x_0) \dots (x - x_k)
	\end{align*}
\end{theorem}

\begin{proof}
	For $i = 0,\dots,k$,
	\begin{align*}
		p_{k + 1}(x_i) & = p_k(x_i) + A_{k + 1} \prod\limits_{j = 0}^{k} (x_i - x_j) \\
                               & = p_k(x_i) + 0
		\marginnote
		{
			$\forall i = 0,\dots,k$, $(x_i - x_i) = 0$.
			Therefore, if $i = j$, $(x_i - x_j) = 0$.
			Therefore, $\prod (x_i - x_j) = 0$
		}
	\end{align*}
	For $i = k + 1$,
	\begin{align*}
		p_{k + 1}(x_{k + 1}) & = p_k(x_{k + 1}) + A_{k + 1} \prod\limits_{j = 0}^{k} (x_{k + 1} - x_j) \\
                                     & = f(x_{k + 1})
	\end{align*}
	where $A_{k + 1}$ can be calculated using $p_k(x_{k + 1})$ and $f(x_{k + 1})$.\\
	Therefore,\\
	For $n = 1$,
	\begin{align*}
		p_0(x) & = A_0 \\
                       & = f(x_0)
	\end{align*}
	For $n = 2$,
	\begin{align*}
		p_1(x) & = p_0(x) + A_1 (x - x_0) \\
                       & = f(x_0) - A_1(x - x_0)  \\
                       & = f(x_1)
	\end{align*}
	Therefore,
	\begin{align*}
		A_1 & = \frac{f(x_1) - f(x_0)}{x_1 - x_0} \\
                    & = f[x_0,x_1]
	\end{align*}
	For $n = 3$,
	\begin{align*}
		p_2(x) & = p_1(x) + A_2 (x - x_0) (x - x_1)                        \\
                       & = f(x_0) + f[x_0,x_1] (x - x_0)                           \\
                       & = f(x_0) + f[x_0,x_1] (x - x_0) + A_2 (x - x_0) (x - x_1) \\
                       & = f(x_2)
	\end{align*}
	Therefore,
	\begin{align*}
		A_2 & = \frac{1}{(x_2 - x_0) (x_2 - x_1)} \left( f(x_2) - f(x_0) - f[x_0,x_1] (x_2 - x_0) \right) \\
                    & = f[x_0,x_1,x_2]
	\end{align*}
	and so on.\\
	In general,
	\begin{align*}
		A_k & = f[x_0,\dots,x_k]
	\end{align*}
\end{proof}

\begin{definition}[Divided difference]
	\begin{align*}
		f[x_0,\dots,x_k] & = \frac{f[x_1,\dots,x_k] - f[x_0,\dots,x_{k - 1]}}{x_k - x_0} \\
		f[x_0]           & = f(x_0)
	\end{align*}
	is called the $k$th order divided difference of $f(x)$.
\end{definition}

\begin{question}
	Given
	\begin{align*}
		k(z) & = \int\limits_{0}^{\frac{\pi}{2}} \frac{\dif x}{\sqrt{1 - (\sin z)^2 (\sin x)^2}}
	\end{align*}
	and
	\begin{align*}
		k(1) & = 1.5709 \\
		k(4) & = 1.5727 \\
		k(6) & = 1.5751
	\end{align*}
	approximate $k(3.5)$.
\end{question}

\begin{solution}
	For the first order divided differences,
	\begin{align*}
		k[x_i] & = k(x_i)
	\end{align*}
	Therefore,
	\begin{align*}
		k[1] & = k(1)   \\
                     & = 1.5709 \\
		k[4] & = k(4)   \\
                     & = 1.5727 \\
		k[6] & = k(6)   \\
                     & = 1.5751
	\end{align*}
	For the second order divided differences,
	\begin{align*}
		k[x_i,x_j] & = \frac{k[i] - k[j]}{i - j}
	\end{align*}
	Therefore,
	\begin{align*}
		k[1,4] & = \frac{k[1] - k[4]}{1 - 4} \\
                       & = \frac{1.5727 - 1.5709}{3} \\
		k[4,6] & = \frac{k[4] - k[6]}{4 - 6} \\
                       & = \frac{1.5751 - 1.5727}{2}
	\end{align*}
	For the third order divided differences,
	\begin{align*}
		k[x_i,x_j,x_k] & = \frac{k[i,j] - k[j,k]}{i - k}
	\end{align*}
	Therefore,
	\begin{align*}
		k[1,4,6] & = \frac{k[1,4] - k[4,6]}{1 - 6}
	\end{align*}
	Hence,
	\begin{align*}
		A_0 & = k[1]   \\
		A_1 & = k[1,4] \\
		A_2 & = k[1,4,6]
	\end{align*}
\end{solution}

\end{document}
